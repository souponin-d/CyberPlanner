# Проект RL-оптимизации расписания поликлиники

Этот проект посвящен применению методов обучения с подкреплением (Reinforcement Learning) для оптимизации расписания приема пациентов в симулированной поликлинике. Цель состоит в минимизации времени ожидания пациентов, простоя врачей и переработки, одновременно максимизируя количество обслуженных пациентов.

## Описание

В рамках проекта разработана среда симуляции поликлиники с использованием библиотеки `Gymnasium`. Агент обучается управлять потоком пациентов, назначая их врачам и при необходимости перенося приемы, чтобы улучшить общую эффективность работы.

Для обучения агента используется алгоритм PPO (Proximal Policy Optimization) из библиотеки `Stable Baselines3`. В качестве базовой (baseline) модели реализована стратегия FIFO (First-In, First-Out) с приоритетом для экстренных пациентов.

## Основные особенности

- **Среда `PolyclinicEnv`**: Пользовательская среда Gymnasium, моделирующая работу поликлиники с несколькими врачами, различными типами пациентов (консультации, экстренные случаи, процедуры) и динамическими прибытиями.
    
- **Гибкая система вознаграждений**: Настраиваемые штрафы за ожидание пациентов, простой врачей, переработку, а также награды за успешные приемы и оптимизацию расписания.
    
- **Агент PPO**: Обучение агента PPO для принятия решений о расписании. Гиперпараметры PPO могут быть оптимизированы с помощью Optuna (сейчас закомментировано, используются фиксированные лучшие параметры).
    
- **Базовая модель FIFO**: Реализация простой, но эффективной стратегии FIFO для сравнения производительности.
    
- **Логирование TensorBoard**: Автоматическое логирование метрик обучения для визуализации прогресса с помощью TensorBoard.
    
- **Оффлайн-построение графиков**: Отдельный скрипт для чтения логов TensorBoard и генерации PNG-графиков для отчетов, что удобно для работы на сервере без графического интерфейса.
    

## Установка

1. **Клонируйте репозиторий** 
2. **Создайте и активируйте виртуальное окружение**:
    ```
    conda create -n rl_polyclinic python=3.12
    conda activate rl_polyclinic
    ```
3. **Установите необходимые библиотеки**:
    ```
    pip install gymnasium numpy stable-baselines3[extra] matplotlib optuna tensorboard
    ```
## Использование

### 1. Обучение модели PPO и оценка

Основной скрипт `polyclinic_rl_project.py` выполняет обучение агента PPO с заданными гиперпараметрами и затем сравнивает его производительность с агентом FIFO.

Для запуска обучения:

```
python polyclinic_rl_project.py
```

После завершения обучения:

- Модель PPO будет сохранена в `ppo_training_1M_steps/ppo_polyclinic_model_1M_steps.zip`.
    
- Логи TensorBoard будут сгенерированы в поддиректории внутри `ppo_training_1M_steps` (например, `ppo_training_1M_steps/PPO_1`).
    
- В консоли будет выведено текстовое сравнение метрик PPO и FIFO.
    

### 2. Просмотр графиков с помощью TensorBoard

После того как вы обучите модель и будут сгенерированы логи TensorBoard, вы можете запустить веб-интерфейс TensorBoard для интерактивной визуализации:

```
tensorboard --logdir ppo_training_1M_steps
```

Откройте в браузере адрес, который будет указан в выводе команды 
### 3. Построение графиков из логов TensorBoard (для оффлайн-отчетов)

Если у вас нет доступа к браузеру на сервере, вы можете использовать скрипт `plot_tensorboard_logs.py` для генерации статических изображений графиков.

Перед запуском скрипта убедитесь, что переменная `LOG_DIR` в `plot_tensorboard_logs.py` правильно указывает на базовую директорию логов (например, `'ppo_training_1M_steps'`).

Запустите скрипт:
```
python plot_tensorboard_logs.py
```
Графики будут сохранены в поддиректории `tensorboard_plots` внутри вашей `LOG_DIR` (например, `ppo_training_1M_steps/tensorboard_plots/`).

## Параметры среды и вознаграждения

### Параметры среды

- `TIME_STEP_MINUTES`: 5 минут (продолжительность одного временного шага симуляции).
    
- `NUM_DOCTORS`: 5 (количество врачей в поликлинике).
    
- `WORK_START_HOUR`: 8, `WORK_END_HOUR`: 21 (рабочий день с 8:00 до 21:00).
    
- `CONSULTATION_DURATION_MINUTES`: 10, `EMERGENCY_DURATION_MINUTES`: 20, `PROCEDURE_DURATION_MINUTES`: 15 (продолжительность приемов для разных типов пациентов).
    
- `MAX_WAITING_ROOM_CAPACITY`: 30 (максимальное количество пациентов, которое может находиться в зале ожидания).
    
- `arrival_rate`: 2.0 (среднее количество пациентов, прибывающих за 5-минутный временной шаг, т.е. 0.4 пациента в минуту).
    
- `emergency_ratio`: 0.2 (доля экстренных пациентов среди всех прибывающих).
    

### Компоненты вознаграждения

- **Награда за успешное назначение пациента**: `+10`.
    
- **Штраф за неудачную попытку назначения пациента**: `-1`.
    
- **Штраф за попытку перенести пациента к тому же самому врачу**: `-0.5`.
    
- **Бонус за успешный перенос, сокращающий время ожидания**: `+5`.
    
- **Бонус за успешный перенос, не сокращающий время ожидания**: `+2`.
    
- **Штраф за неудачную попытку перепланирования (не найден слот)**: `-5`.
    
- **Штраф за попытку перепланирования при отсутствии пациента для переноса**: `-1`.
    
- **Штраф за ожидание пациента**: `-5.0` за каждого ожидающего пациента на каждом временном шаге.
    
- **Штраф за простой врача**: `-0.05` за каждый временной шаг простоя.
    
- **Штраф за переработку врача**: `-2` за каждый временной шаг переработки.
    
- **Штраф за необслуженного пациента в конце дня**: `-5`.
    

## Результаты и сравнение

По завершении обучения и оценки скрипт выведет сравнительную таблицу метрик для агентов PPO и FIFO, а также сохранит графики их производительности в указанной директории. Оценка включает среднюю общую награду, среднее время ожидания пациентов, процент простоя врачей, процент переработки, общее число обслуженных и прибывших пациентов.
